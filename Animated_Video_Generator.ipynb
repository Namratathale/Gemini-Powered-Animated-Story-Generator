{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3iQ41790ZHE",
    "outputId": "f4ae996a-f881-4d50-a5ef-0ad6fbd1190a"
   },
   "outputs": [],
   "source": [
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLFJUeYu0iEa"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtyVAIRZ0w76"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "os.environ['API_KEY'] = userdata.get('API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_8t639M33Xo",
    "outputId": "9781c87c-3d31-4b72-d0ec-fd8a73d3ec21"
   },
   "outputs": [],
   "source": [
    "!apt-get update -qq && apt-get install -qq locales\n",
    "!locale-gen en_US.UTF-8\n",
    "!update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8\n",
    "\n",
    "!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
    "%pip install -q google-generativeai moviepy Pillow\n",
    "%pip install -q nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bID3M79y33HU",
    "outputId": "18092bd3-74d9-43b9-bcb3-c6c6fe101577"
   },
   "outputs": [],
   "source": [
    "#core data processing\n",
    "import json\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "#image handling\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "#for video & audio\n",
    "from moviepy.editor import ImageClip, AudioFileClip, CompositeVideoClip, concatenate_videoclips\n",
    "\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "import typing_extensions as typing\n",
    "\n",
    "#aasync support\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import contextlib\n",
    "import wave\n",
    "\n",
    "#Google GenAI\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6NAgJLwnfUU",
    "outputId": "f0b18da6-b355-47c4-8da5-597ba4daf353"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Ensure the API key is loaded from Colab's userdata secrets\n",
    "api_key = userdata.get('API_KEY')\n",
    "# You can optionally set it as an environment variable as well, though not strictly necessary for the client constructor\n",
    "os.environ['API_KEY'] = api_key\n",
    "\n",
    "# Initialize the client with the API key\n",
    "# Removed the 'version' parameter as it caused a TypeError\n",
    "client = genai.Client(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Changed the model name to a standard content generation model\n",
    "MODEL = \"models/gemini-2.0-flash-exp\" # Keep the model name as it's common for v1\n",
    "\n",
    "IMAGE_MODEL_ID = \"imagen-3.0-generate-002\"\n",
    "\n",
    "class StorySegment(typing.TypedDict):\n",
    "  image_prompt: str\n",
    "  audio_text: str\n",
    "  character_description:str\n",
    "\n",
    "\n",
    "class StoryResponse(typing.TypedDict):\n",
    "  complete_story: list[StorySegment]\n",
    "  pages:int\n",
    "\n",
    "def generate_story_sequence(complete_story:str, pages:int) -> list[StoryResponse]:\n",
    "  response = client.models.generate_content(\n",
    "      model=MODEL,\n",
    "      contents=f'''you are an animation video producer. Generate a story sequence about {complete_story} in {pages} scenes (with interactions and characters), 1 sec each scene. Write:\n",
    "\n",
    "image_prompt:(define art style for kids animation(consistent for all the characters), no violence) a full description of the scene, the characters in it, and the background in 20 words or less. Progressively shift the scene as the story advances.\n",
    "audio_text: a one-sentence dialogue/narration for the scene.\n",
    "character_description: no people ever, only animals and objects. Describe all characters (consistent names, features, clothing, etc.) with an art style reference (e.g., \"Pixar style,\" \"photorealistic,\" \"Ghibli\") in 30 words or less.\n",
    "''',\n",
    "      config={\n",
    "          'response_mime_type':'application/json',\n",
    "          'response_schema': list[StoryResponse]\n",
    "      }\n",
    "  )\n",
    "  try:\n",
    "    story_data_text = response.text\n",
    "    story_data_list = json.loads(story_data_text)\n",
    "    if isinstance(story_data_list,list) and len(story_data_list)>0:\n",
    "      story_data = story_data_list[0]\n",
    "      # Returning complete_story and character_description as the function signature suggests\n",
    "      return story_data.get('complete_story',[]), story_data.get('character_description', {})\n",
    "    else:\n",
    "      return [], {} # Return empty list and empty dict if response is not as expected\n",
    "  except (KeyError, TypeError, IndexError, json.JSONDecodeError) as e:\n",
    "    print(f\"Error parsing story data: {e}\")\n",
    "    return [], {} # Return empty list and empty dict on error\n",
    "\n",
    "\n",
    "theme = \"Supa Strikers win SuperLeague and atlast Shakes meet his dadðŸŒŸ\"\n",
    "num_scenes = 30\n",
    "\n",
    "story_segments, character_description = generate_story_sequence(theme, num_scenes)\n",
    "print(json.dumps(story_segments,indent=3))\n",
    "# Optionally print character description\n",
    "# print(json.dumps(character_description, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PV0H8xNLe41N"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
    "    with wave.open(filename, \"wb\") as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(sample_width)\n",
    "        wf.setframerate(rate)\n",
    "        yield wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBiXjNmdrJmI"
   },
   "outputs": [],
   "source": [
    "# --- Cell 2: Definitions and setup ---\n",
    "temp_audio_files = []  # To track temporary audio files\n",
    "temp_image_files = []  # To track temporary image files\n",
    "video_clips = []       # To store individual video clips for each scene\n",
    "\n",
    "def generate_audio_live(api_text, output_filename):\n",
    "    import asyncio\n",
    "    collected_audio = bytearray()\n",
    "\n",
    "    async def _generate():\n",
    "        config = {\n",
    "            \"response_modalities\": [\"AUDIO\"]\n",
    "        }\n",
    "        # Connect to the Live API using the client already initialized above.\n",
    "        async with client.aio.live.connect(model=MODEL, config=config) as session:\n",
    "            # Send the audio_text prompt; mark as end_of_turn.\n",
    "            await session.send(input=api_text, end_of_turn=True)\n",
    "            # Collect audio data as it streams in.\n",
    "            async for response in session.receive():\n",
    "                if response.data:\n",
    "                    collected_audio.extend(response.data)\n",
    "        return bytes(collected_audio)\n",
    "\n",
    "    # Run the async function and collect the audio bytes.\n",
    "    audio_bytes = asyncio.run(_generate())\n",
    "    # Write the collected audio bytes into a WAV file using the helper.\n",
    "    with wave_file(output_filename) as wf:\n",
    "        wf.writeframes(audio_bytes)\n",
    "    return output_filename\n",
    "\n",
    "\n",
    "\n",
    "# Note: Use a system instruction to prevent common AI responses and ensure natural narration\n",
    "audio_negative_prompt = \"don't say OK , I will do this or that, just only read this story using voice expressions without introductions or ending ,more segments are coming ,don't say OK , I will do this or that:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XaO0mFA67s5Y",
    "outputId": "91101879-97fd-4095-c6dc-a65c35867fdd"
   },
   "outputs": [],
   "source": [
    "# --- Cell 3: Main processing loop ---\n",
    "for i, segment in enumerate(story_segments):\n",
    "    # Retrieve details for the current scene.\n",
    "    image_prompt = segment['image_prompt']\n",
    "    audio_text =  audio_negative_prompt + segment['audio_text']\n",
    "    audio_text_prompt = segment['audio_text']\n",
    "    char_desc = segment['character_description']\n",
    "    print(f\"Processing scene {i}:\")\n",
    "    print(\"Image Prompt:\", image_prompt)\n",
    "    print(\"Audio Text:\", audio_text_prompt)\n",
    "    print(\"Character Description:\", char_desc)\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Image Generation using Google Imagen\n",
    "    # -------------------------\n",
    "    # Modify the combined_prompt here\n",
    "    combined_prompt = \"detailed children book animation style \" + image_prompt + \" \" + char_desc\n",
    "\n",
    "    result = client.models.generate_images(\n",
    "        model=IMAGE_MODEL_ID,\n",
    "        prompt=combined_prompt, # The prompt that might need modification\n",
    "        config={\n",
    "            \"number_of_images\": 1,\n",
    "            \"output_mime_type\": \"image/jpeg\",\n",
    "            \"person_generation\": \"DONT_ALLOW\", # This setting helps avoid issues with generating people\n",
    "            \"aspect_ratio\": \"1:1\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if not result.generated_images:\n",
    "            raise ValueError(\"No images were generated. The prompt might have been flagged as harmful. Please modify your prompt and try again.\")\n",
    "        for generated_image in result.generated_images:\n",
    "            image = Image.open(BytesIO(generated_image.image.image_bytes))\n",
    "    except Exception as e:\n",
    "        print(\"Image generation failed \", e)\n",
    "        # Consider adding a placeholder image or skipping this segment if image generation fails\n",
    "        # For example:\n",
    "        # image = None # Or load a default image\n",
    "        continue # Skip to the next segment\n",
    "\n",
    "    if image: # Only proceed if an image was successfully generated\n",
    "        image_path = f\"image_{i}.png\"\n",
    "        image.save(image_path)\n",
    "        temp_image_files.append(image_path)\n",
    "        display(image)\n",
    "\n",
    "        # -------------------------\n",
    "        # Audio Generation using Google Live API\n",
    "        # -------------------------\n",
    "        audio_path = f\"audio_{i}.wav\"\n",
    "        # The generate_audio_live function now uses LIVE_AUDIO_MODEL\n",
    "        audio_path = generate_audio_live(audio_text, audio_path)\n",
    "        temp_audio_files.append(audio_path)\n",
    "\n",
    "\n",
    "        # -------------------------\n",
    "        # Create Video Clip (Image + Audio)\n",
    "        # -------------------------\n",
    "        audio_clip = AudioFileClip(audio_path)\n",
    "\n",
    "        # Convert PIL Image to numpy array\n",
    "        np_image = np.array(image)\n",
    "\n",
    "        # Create ImageClip (size is inferred from np_image)\n",
    "        image_clip = ImageClip(np_image).set_duration(audio_clip.duration)\n",
    "\n",
    "        # Store composite clip with audio in memory\n",
    "        composite_clip = CompositeVideoClip([image_clip]).set_audio(audio_clip)\n",
    "        video_clips.append(composite_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m57K5yl09I_7"
   },
   "outputs": [],
   "source": [
    "final_video = concatenate_videoclips(video_clips)\n",
    "output_filename = f\"{int(time.time())}_output_video.mp4\"\n",
    "print(\"Writing final video to\", output_filename)\n",
    "final_video.write_videofile(output_filename, fps=24)\n",
    "\n",
    "# Display the video in the notebook\n",
    "def show_video(video_path):\n",
    "    \"\"\"Display video in notebook\"\"\"\n",
    "    video_file = open(video_path, \"rb\")\n",
    "    video_bytes = video_file.read()\n",
    "    video_b64 = b64encode(video_bytes).decode()\n",
    "    video_tag = f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\"></video>'\n",
    "    return HTML(video_tag)\n",
    "\n",
    "# Show the video\n",
    "display(show_video(output_filename))\n",
    "\n",
    "# Cleanup: Close video clips and remove temporary files\n",
    "final_video.close()\n",
    "for clip in video_clips:\n",
    "    clip.close()\n",
    "for file in temp_audio_files:\n",
    "    os.remove(file)\n",
    "for file in temp_image_files:\n",
    "    os.remove(file)\n",
    "\n",
    "\n",
    "# A video player will appear below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqmIr77F9I1k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "571_CTy_9Io2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
